# 分布式生产环境的搭建—非高可用环境搭建

本篇主要介绍使用四台机器搭建`Hadoop`分布式环境，由于是实验学习环境，所以暂时不对着急搭建分布式的高可用方案，先从最基本的一步一步搭建，从中穿插介绍一些个人的理解以及查阅的资料。

## 一、Hadoop版本选择

|   序号  | 组件名称 | 组件版本 |
|:-------| :------- | :-----  |
| 1      | hadoop  |   2.7.5 |
| 2      |    JDK     |  jdk1.8.0_65  |

## 二、部署构架环境信息说明

本次实验使用到4台服务器，搭建生产环境并非必须4台服务器，三台服务器即可，因为是实验测试环境，为了方便测试性能以及功能的调试，便于学习组件。如下表所示是每一个服务器的所运行的组件信息


|序号| 服务器IP | 功能角色 |
|:---:|:----:|:----:|
| 1 | 192.168.208.60(namenode1)| NameNode、ResourceManager |
| 2 | 192.168.208.61(datanode1)| SecondaryNameNode、NodeManager、DataNode |
| 3 | 192.168.208.62(datanode2)| NodeManager、DataNode |
| 4 | 192.168.208.63(datanode3)| NodeManager、DataNode |


 如上表示，各个功能角色作用描述如下
- `NameNode`  负责存储`HDFS`文件系统的元数据信息，里面记录了文件系统中所有文件的大小以及块信息
- `ResourceManager` 是`Yarn`资源管理器中的一个角色，实现统计与分配`Hadoop`集群上计算资源，主要是`CPU`和`MEM`
- `NodeManager` 实现自己本机的资源管理与监控
- `DataNode`    存储数据副本进程
- `SecondaryNameNode` 实现定时合并`HDFS`文件系统的`fsimage`和`edits`文件，在出现问题时，我们可以将该处的文件覆盖至namenode中，重新拉起namenode，但是以为合并是定时合并的，所以肯定会有一小部分数据丢失的问题

## 三、分布式环境搭建步骤

基于以上的环境介绍，现在开始搭建分布式环境，具体的步骤如下，总的来说具体的步骤可以参考[官方搭建文档](http://hadoop.apache.org/docs/r2.7.5/hadoop-project-dist/hadoop-common/ClusterSetup.html)

### 3.1、安装与部署JDK

该步骤比较简单，再次我们选择`JDK`版本为`1.8`，至于官方对`JDK`版本的要求，可以[查询该文档](https://wiki.apache.org/hadoop/HadoopJavaVersions)，有详细的说明。

```bash
Shell> curl -O http://fdoc.epoint.com.cn:3366/JDK/jdk-8u65-linux-x64.tar.gz
Shell> tar -zxvf jdk-8u65-linux-x64.tar.gz -C /opt/
Shell> ln -s /opt/jdk1.8.0_65 /usr/local/jdk
Shell> vim /etc/profile #编辑配置文件，添加如下内容
#######省略部分输出#########
# JAVA JDK Setting
export JAVA_HOME=/usr/local/jdk
export PATH=$PATH:$JAVA_HOME/bin
Shell> source /etc/profile

```
配置完成后请记住`source`配置文件。该步骤需要在三台机器上都要执行



### 3.2、配置主机间相互信任与主机名称访问

配置主机名，方法非常简单，编辑修改`/etc/hostname`修改为上表中计划配置的主机名称，配置完主机名称修改每台主机的`/etc/hosts`文件加入主机名与`IP`地址的对应关系，内容如下所示
```bash
[root@namenode1 ~]# cat /etc/hosts
......省略已有的默认配置..........
192.168.208.60 namenode1
192.168.208.61 datanode1
192.168.208.62 datanode2
192.168.208.63 datanode3
```

完成上述配置后我们开始配置主机之间的相互信任，配置很简单，原理也比较简单，就是在本机上生成自己的公钥，在将公钥发送至其他机器上，下次在登录其他机器上时，验证登录机器上的公钥和私钥即可证明身份

```bash
[root@namenode1 ~]$ ssh-keygen -t rsa # 请注意执行该命令的机器是在namenode1机器上
[root@namenode1 ~]$ ssh-keygen -t rsa
Generating public/private rsa key pair.
Enter file in which to save the key (/home/ijarvis/.ssh/id_rsa): #指定私钥生成地址，默认即可
Enter passphrase (empty for no passphrase): #输入秘钥认证密码，实现免密码登录直接回车即可
Enter same passphrase again:
Your identification has been saved in /home/ijarvis/.ssh/id_rsa.
Your public key has been saved in /home/ijarvis/.ssh/id_rsa.pub.
The key fingerprint is:
SHA256:P5g1xDuo8sfY5sHpI5tJIv7ylcIujFOmn5ZcINt5aog ijarvis@ijarvis
The key's randomart image is:
+---[RSA 2048]----+
|                 |
|         .       |
|          o      |
|. .      o .     |
| + o    S =      |
|. =.o  + * o     |
|.O.== =+* o      |
|E.X+ Bo+*. .     |
| =+=+ =*o.       |
+----[SHA256]-----+
[root@namenode1 ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub root@datanode1 # 将namenode1的公钥发送追加至datanode1主机中
[root@namenode1 ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub root@datanode2 
[root@namenode1 ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub root@datanode3 # 如上步骤所示，将自己的key配置到其他机器上
```

### 3.3、配置Hadoop集群配置文件

修改与配置`Hadoop`配置文件很简单，在初始状态下集群内的文件配置都是一样的，只要在一台机器上修改好其他的机器使用`scp`配置替换就好了。

首先配置`Hadoop`的环境变量信息，这些变量信息指定了`Hadoop`集群的配置文件位置等信息，我们这里现在`namenode1`机器上配置，然后在同步至其他的`datanode`上即可，方法如下
```bash
Shell> vim /etc/profile
.......省略部分配置......
# Hadoop Install Setting
export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin
```

`Hadoop`的配置文件可以分为如下几个，作用如下


```bash
hadoop-env.sh  # Hadoop的启动环境构建脚本，类似tomcat的catalina.sh脚本一样
core-site.xml  # 配置Hadoop启用的文件系统模式等信息，是Hadoop的核心配置
hdfs-site.xml  # HDFS文件系统存储配置文件
yarn-site.xml  # 配置yarn环境下ResourceManager和NodeManager的配置，该部分的配置主要涉及到分布式计算框架的属性配置
mapred-site.xml # 该配置文件主要涉及到MapReduce的job运行以及MapReduce JobHistory Server属性配置
slaves         # 该文件中定义了HDFS集群中datanode，每行一个主机，在稍后的启动脚本中需要使用到
```

对于`hadoop-env.sh`配置文件，需要在文件开始位置加入`JAVA_HOME`变量，虽然在脚本中使用了`${JAVA_HOME}`来取的环境变量，但是实际笔者在测试时并不能正确拿到该变量，所以只能强制指定一下。

```bash
Shell> vim hadoop-env.sh
# Set Hadoop-specific environment variables here.

# The only required environment variable is JAVA_HOME.  All others are
# optional.  When running a distributed configuration it is best to
# set JAVA_HOME in this file, so that it is correctly defined on
# remote nodes.

# The java implementation to use.
JAVA_HOME=/usr/local/jdk   # 这里就是我们需要去修改的
export JAVA_HOME=${JAVA_HOME}

.........省略剩余配置输出.......

```

对于`core-site.xml`我们需要告诉`Hadoop`使用的文件系统以及副本集
```bash
Shell> vim core-site.xml
<configuration>
  <!--  配置hadoop的临时文件目录，该目录下会存储fsimage和edits -->
  <property>
    <name>hadoop.tmp.dir</name>
    <value>/usr/local/hadoop/tmp</value>
  </property>
  <!-- 配置hadoop启用的文件系统 -->
  <property>
    <name>fs.default.name</name>
    <value>hdfs://namenode1:9000</value>
  </property>
  <property>
    <name>hadoop.proxyuser.root.groups</name>
    <value>*</value>
  </property>
  <property>
    <name>hadoop.proxyuser.root.hosts</name>
    <value>*</value>
  </property>
</configuration>
```

配置`hdfs-site.xml`文件，在该文件中主要指定配置`文件系统的副本数，namenode以及secondarynamenode的配置`
```bash
<configuration>
  <!-- hdfs文件系统的管理界面地址 -->
  <property>
    <name>dfs.http.address</name>
    <value>namenode1:50070</value>
  </property>
  <property>
    <name>dfs.namenode.secondary.http-address</name>
    <value>datanode1:50090</value>
  </property>
  <!-- 分布式文件系统存储的副本数 -->
  <property>
    <name>dfs.replication</name>
    <value>3</value>
  </property>
  <!-- hadoop文件系统的数据存储位置 -->
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>/opt/hadoopdata</value>
  </property>
</configuration>
```

配置`yarn-site.xml`文件，在该文件中主要指定`Yarn`资源管理系统的配置，例如指定`ResourceManager`组件运行的位置等等信息
```bash
<configuration>
<!-- Site specific YARN configuration properties -->
  <property>
    <name>yarn.resourcemanager.address</name>
    <value>namenode1:8032</value>
  </property>
  <property>
    <name>yarn.resourcemanager.scheduler.address</name>
    <value>namenode1:8030</value>
  </property>
  <property>
    <name>yarn.resourcemanager.webapp.address</name>
    <value>namenode1:8088</value>
  </property>
  <property>
    <name>yarn.resourcemanager.resource-tracker.address</name>
    <value>namenode1:8031</value>
  </property>
  <property>
    <name>yarn.resourcemanager.admin.address</name>
    <value>namenode1:8033</value>
  </property>
</configuration>

```

配置`mapred-site.xml`文件，在该文件中主要指定`MapReduce`程序使用的框架信息等等
```bash
<configuration>
  <property>
    <name>mapred.job.tracker</name>
    <value>namenode:9001</value>
  </property>
  <property>
    <name>mapreduce.jobhistory.address</name>
    <value>namenode:10020</value>
  </property>
  <property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>namenode:19888</value>
  </property>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
</configuration>
```

配置`slaves`文件，该文件中指定运行存储节点的机器名，每一行一个
```bash
datanode1
datanode2
datanode3
```


如上所示配置完成后，使用`scp`命令将`${HADOOP_HOME}/etc`目录发送至其他集群节点上即可
```bash
Shell> scp -r $HADOOP_HOME/etc root@datanode1:/usr/local/hadoop/   #覆盖前请删除目标机器上的文件，不然不一定会覆盖
Shell> scp -r $HADOOP_HOME/etc root@datanode2:/usr/local/hadoop/   #覆盖前请删除目标机器上的文件，不然不一定会覆盖
Shell> scp -r $HADOOP_HOME/etc root@datanode3:/usr/local/hadoop/   #覆盖前请删除目标机器上的文件，不然不一定会覆盖
```


### 3.4 启动Hadoop集群

启动集群的方法有很多，你可以一个个进程按照顺序去启动，也可以使用官方准备的脚本`start-all.sh`去启动，但是其实还是在调用`start-yarn.sh`和`start-dfs.sh`。在启动前先使用格式化`Namenode`文件系统
```bash
Shell> hdfs namenode -format #格式化元数据库
Shell> start-all.sh #启动Hadoop集群
```



## 四、配置完成后的相关验证



配置完成`Hadoop`集群后我们可以在浏览器中查看`http://192.168.208.60:50070/`该地址即可使用可视化的页面查看`hdfs`管理页面


![](images/hdfsweb.png)








































